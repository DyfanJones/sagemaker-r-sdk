% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/amazon_object2vec.R
\name{Object2Vec}
\alias{Object2Vec}
\title{A general-purpose neural embedding algorithm that is highly customizable.}
\description{
It can learn low-dimensional dense embeddings of high-dimensional objects. The embeddings
             are learned in a way that preserves the semantics of the relationship between pairs of
             objects in the original space in the embedding space.
}
\section{Super classes}{
\code{\link[R6sagemaker:EstimatorBase]{R6sagemaker::EstimatorBase}} -> \code{\link[R6sagemaker:AmazonAlgorithmEstimatorBase]{R6sagemaker::AmazonAlgorithmEstimatorBase}} -> \code{Object2Vec}
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{repo_name}}{sagemaker repo name for framework}

\item{\code{repo_version}}{version of framework}

\item{\code{MINI_BATCH_SIZE}}{The size of each mini-batch to use when training.}
}
\if{html}{\out{</div>}}
}
\section{Active bindings}{
\if{html}{\out{<div class="r6-active-bindings">}}
\describe{
\item{\code{epochs}}{Total number of epochs for SGD training}

\item{\code{enc_dim}}{Dimension of the output of the embedding layer}

\item{\code{mini_batch_size}}{mini batch size for SGD training}

\item{\code{early_stopping_patience}}{The allowed number of consecutive epochs without
improvement before early stopping is applied}

\item{\code{early_stopping_tolerance}}{The value used to determine whether the algorithm
has made improvement between two consecutive epochs for early stopping}

\item{\code{dropout}}{Dropout probability on network layers}

\item{\code{weight_decay}}{Weight decay parameter during optimization}

\item{\code{bucket_width}}{The allowed difference between data sequence length when bucketing is enabled}

\item{\code{num_classes}}{Number of classes for classification}

\item{\code{mlp_layers}}{Number of MLP layers in the network}

\item{\code{mlp_dim}}{Dimension of the output of MLP layer}

\item{\code{mlp_activation}}{Type of activation function for the MLP layer}

\item{\code{output_layer}}{Type of output layer}

\item{\code{optimizer}}{Type of optimizer for training}

\item{\code{learning_rate}}{Learning rate for SGD training}

\item{\code{negative_sampling_rate}}{Negative sampling rate}

\item{\code{comparator_list}}{Customization of comparator operator}

\item{\code{tied_token_embedding_weight}}{Tying of token embedding layer weight}

\item{\code{token_embedding_storage_type}}{Type of token embedding storage}

\item{\code{enc0_network}}{Network model of encoder "enc0"}

\item{\code{enc1_network}}{Network model of encoder "enc1"}

\item{\code{enc0_cnn_filter_width}}{CNN filter width}

\item{\code{enc1_cnn_filter_width}}{CNN filter width}

\item{\code{enc0_max_seq_len}}{Maximum sequence length}

\item{\code{enc1_max_seq_len}}{Maximum sequence length}

\item{\code{enc0_token_embedding_dim}}{Output dimension of token embedding layer}

\item{\code{enc1_token_embedding_dim}}{Output dimension of token embedding layer}

\item{\code{enc0_vocab_size}}{Vocabulary size of tokens}

\item{\code{enc1_vocab_size}}{Vocabulary size of tokens}

\item{\code{enc0_layers}}{Number of layers in encoder}

\item{\code{enc1_layers}}{Number of layers in encoder}

\item{\code{enc0_freeze_pretrained_embedding}}{Freeze pretrained embedding weights}

\item{\code{enc1_freeze_pretrained_embedding}}{Freeze pretrained embedding weights}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{Object2Vec$new()}}
\item \href{#method-create_model}{\code{Object2Vec$create_model()}}
\item \href{#method-.prepare_for_training}{\code{Object2Vec$.prepare_for_training()}}
\item \href{#method-clone}{\code{Object2Vec$clone()}}
}
}
\if{html}{
\out{<details ><summary>Inherited methods</summary>}
\itemize{
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="EstimatorBase" data-id="attach">}\href{../../R6sagemaker/html/EstimatorBase.html#method-attach}{\code{R6sagemaker::EstimatorBase$attach()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="EstimatorBase" data-id="compile_model">}\href{../../R6sagemaker/html/EstimatorBase.html#method-compile_model}{\code{R6sagemaker::EstimatorBase$compile_model()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="EstimatorBase" data-id="delete_endpoint">}\href{../../R6sagemaker/html/EstimatorBase.html#method-delete_endpoint}{\code{R6sagemaker::EstimatorBase$delete_endpoint()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="EstimatorBase" data-id="deploy">}\href{../../R6sagemaker/html/EstimatorBase.html#method-deploy}{\code{R6sagemaker::EstimatorBase$deploy()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="EstimatorBase" data-id="enable_network_isolation">}\href{../../R6sagemaker/html/EstimatorBase.html#method-enable_network_isolation}{\code{R6sagemaker::EstimatorBase$enable_network_isolation()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="EstimatorBase" data-id="get_vpc_config">}\href{../../R6sagemaker/html/EstimatorBase.html#method-get_vpc_config}{\code{R6sagemaker::EstimatorBase$get_vpc_config()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="EstimatorBase" data-id="latest_job_debugger_artifacts_path">}\href{../../R6sagemaker/html/EstimatorBase.html#method-latest_job_debugger_artifacts_path}{\code{R6sagemaker::EstimatorBase$latest_job_debugger_artifacts_path()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="EstimatorBase" data-id="latest_job_tensorboard_artifacts_path">}\href{../../R6sagemaker/html/EstimatorBase.html#method-latest_job_tensorboard_artifacts_path}{\code{R6sagemaker::EstimatorBase$latest_job_tensorboard_artifacts_path()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="EstimatorBase" data-id="logs">}\href{../../R6sagemaker/html/EstimatorBase.html#method-logs}{\code{R6sagemaker::EstimatorBase$logs()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="EstimatorBase" data-id="print">}\href{../../R6sagemaker/html/EstimatorBase.html#method-print}{\code{R6sagemaker::EstimatorBase$print()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="EstimatorBase" data-id="transformer">}\href{../../R6sagemaker/html/EstimatorBase.html#method-transformer}{\code{R6sagemaker::EstimatorBase$transformer()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="AmazonAlgorithmEstimatorBase" data-id="fit">}\href{../../R6sagemaker/html/AmazonAlgorithmEstimatorBase.html#method-fit}{\code{R6sagemaker::AmazonAlgorithmEstimatorBase$fit()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="AmazonAlgorithmEstimatorBase" data-id="hyperparameters">}\href{../../R6sagemaker/html/AmazonAlgorithmEstimatorBase.html#method-hyperparameters}{\code{R6sagemaker::AmazonAlgorithmEstimatorBase$hyperparameters()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="AmazonAlgorithmEstimatorBase" data-id="prepare_workflow_for_training">}\href{../../R6sagemaker/html/AmazonAlgorithmEstimatorBase.html#method-prepare_workflow_for_training}{\code{R6sagemaker::AmazonAlgorithmEstimatorBase$prepare_workflow_for_training()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="AmazonAlgorithmEstimatorBase" data-id="record_set">}\href{../../R6sagemaker/html/AmazonAlgorithmEstimatorBase.html#method-record_set}{\code{R6sagemaker::AmazonAlgorithmEstimatorBase$record_set()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="AmazonAlgorithmEstimatorBase" data-id="training_image_uri">}\href{../../R6sagemaker/html/AmazonAlgorithmEstimatorBase.html#method-training_image_uri}{\code{R6sagemaker::AmazonAlgorithmEstimatorBase$training_image_uri()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="R6sagemaker" data-topic="AmazonAlgorithmEstimatorBase" data-id="wait">}\href{../../R6sagemaker/html/AmazonAlgorithmEstimatorBase.html#method-wait}{\code{R6sagemaker::AmazonAlgorithmEstimatorBase$wait()}}\out{</span>}
}
\out{</details>}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Object2Vec is :class:`Estimator` used for anomaly detection.
             This Estimator may be fit via calls to
             :meth:`~sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase.fit`.
             There is an utility
             :meth:`~sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase.record_set`
             that can be used to upload data to S3 and creates
             :class:`~sagemaker.amazon.amazon_estimator.RecordSet` to be passed to
             the `fit` call.
             After this Estimator is fit, model data is stored in S3. The model
             may be deployed to an Amazon SageMaker Endpoint by invoking
             :meth:`~sagemaker.amazon.estimator.EstimatorBase.deploy`. As well as
             deploying an Endpoint, deploy returns a
             :class:`~sagemaker.amazon.Predictor` object that can be used for
             inference calls using the trained model hosted in the SageMaker
             Endpoint.
             Object2Vec Estimators can be configured by setting hyperparameters.
             The available hyperparameters for Object2Vec are documented below.
             For further information on the AWS Object2Vec algorithm, please
             consult AWS technical documentation:
             https://docs.aws.amazon.com/sagemaker/latest/dg/object2vec.html
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Object2Vec$new(
  role,
  instance_count,
  instance_type,
  epochs,
  enc0_max_seq_len,
  enc0_vocab_size,
  enc_dim = NULL,
  mini_batch_size = NULL,
  early_stopping_patience = NULL,
  early_stopping_tolerance = NULL,
  dropout = NULL,
  weight_decay = NULL,
  bucket_width = NULL,
  num_classes = NULL,
  mlp_layers = NULL,
  mlp_dim = NULL,
  mlp_activation = NULL,
  output_layer = NULL,
  optimizer = NULL,
  learning_rate = NULL,
  negative_sampling_rate = NULL,
  comparator_list = NULL,
  tied_token_embedding_weight = NULL,
  token_embedding_storage_type = NULL,
  enc0_network = NULL,
  enc1_network = NULL,
  enc0_cnn_filter_width = NULL,
  enc1_cnn_filter_width = NULL,
  enc1_max_seq_len = NULL,
  enc0_token_embedding_dim = NULL,
  enc1_token_embedding_dim = NULL,
  enc1_vocab_size = NULL,
  enc0_layers = NULL,
  enc1_layers = NULL,
  enc0_freeze_pretrained_embedding = NULL,
  enc1_freeze_pretrained_embedding = NULL,
  ...
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{role}}{(str): An AWS IAM role (either name or full ARN). The Amazon
SageMaker training jobs and APIs that create Amazon SageMaker
endpoints use this role to access training data and model
artifacts. After the endpoint is created, the inference code
might use the IAM role, if accessing AWS resource.}

\item{\code{instance_count}}{(int): Number of Amazon EC2 instances to use
for training.}

\item{\code{instance_type}}{(str): Type of EC2 instance to use for training,
for example, 'ml.c4.xlarge'.}

\item{\code{epochs}}{(int): Total number of epochs for SGD training}

\item{\code{enc0_max_seq_len}}{(int): Maximum sequence length}

\item{\code{enc0_vocab_size}}{(int): Vocabulary size of tokens}

\item{\code{enc_dim}}{(int): Optional. Dimension of the output of the embedding
layer}

\item{\code{mini_batch_size}}{(int): Optional. mini batch size for SGD training}

\item{\code{early_stopping_patience}}{(int): Optional. The allowed number of
consecutive epochs without improvement before early stopping is
applied}

\item{\code{early_stopping_tolerance}}{(float): Optional. The value used to
determine whether the algorithm has made improvement between two
consecutive epochs for early stopping}

\item{\code{dropout}}{(float): Optional. Dropout probability on network layers}

\item{\code{weight_decay}}{(float): Optional. Weight decay parameter during
optimization}

\item{\code{bucket_width}}{(int): Optional. The allowed difference between data
sequence length when bucketing is enabled}

\item{\code{num_classes}}{(int): Optional. Number of classes for classification}

\item{\code{mlp_layers}}{(int): Optional. Number of MLP layers in the network}

\item{\code{mlp_dim}}{(int): Optional. Dimension of the output of MLP layer}

\item{\code{mlp_activation}}{(str): Optional. Type of activation function for the
MLP layer}

\item{\code{output_layer}}{(str): Optional. Type of output layer}

\item{\code{optimizer}}{(str): Optional. Type of optimizer for training}

\item{\code{learning_rate}}{(float): Optional. Learning rate for SGD training}

\item{\code{negative_sampling_rate}}{(int): Optional. Negative sampling rate}

\item{\code{comparator_list}}{(str): Optional. Customization of comparator
operator}

\item{\code{tied_token_embedding_weight}}{(bool): Optional. Tying of token
embedding layer weight}

\item{\code{token_embedding_storage_type}}{(str): Optional. Type of token
embedding storage}

\item{\code{enc0_network}}{(str): Optional. Network model of encoder "enc0"}

\item{\code{enc1_network}}{(str): Optional. Network model of encoder "enc1"}

\item{\code{enc0_cnn_filter_width}}{(int): Optional. CNN filter width}

\item{\code{enc1_cnn_filter_width}}{(int): Optional. CNN filter width}

\item{\code{enc1_max_seq_len}}{(int): Optional. Maximum sequence length}

\item{\code{enc0_token_embedding_dim}}{(int): Optional. Output dimension of token
embedding layer}

\item{\code{enc1_token_embedding_dim}}{(int): Optional. Output dimension of token
embedding layer}

\item{\code{enc1_vocab_size}}{(int): Optional. Vocabulary size of tokens}

\item{\code{enc0_layers}}{(int): Optional. Number of layers in encoder}

\item{\code{enc1_layers}}{(int): Optional. Number of layers in encoder}

\item{\code{enc0_freeze_pretrained_embedding}}{(bool): Optional. Freeze pretrained
embedding weights}

\item{\code{enc1_freeze_pretrained_embedding}}{(bool): Optional. Freeze pretrained
embedding weights}

\item{\code{...}}{: base class keyword argument values.}

\item{\code{training}}{(ignored for regression problems)}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-create_model"></a>}}
\if{latex}{\out{\hypertarget{method-create_model}{}}}
\subsection{Method \code{create_model()}}{
Return a :class:`~sagemaker.amazon.Object2VecModel` referencing the
             latest s3 model data produced by this Estimator.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Object2Vec$create_model(vpc_config_override = "VPC_CONFIG_DEFAULT", ...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{vpc_config_override}}{(dict[str, list[str]]): Optional override for VpcConfig set on
the model. Default: use subnets and security groups from this Estimator.
* 'Subnets' (list[str]): List of subnet ids.
* 'SecurityGroupIds' (list[str]): List of security group ids.}

\item{\code{...}}{: Additional kwargs passed to the Object2VecModel constructor.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-.prepare_for_training"></a>}}
\if{latex}{\out{\hypertarget{method-.prepare_for_training}{}}}
\subsection{Method \code{.prepare_for_training()}}{
Set hyperparameters needed for training. This method will also
             validate ``source_dir``.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Object2Vec$.prepare_for_training(
  records,
  mini_batch_size = NULL,
  job_name = NULL
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{records}}{(RecordSet) – The records to train this Estimator on.}

\item{\code{mini_batch_size}}{(int or None) – The size of each mini-batch to use
when training. If None, a default value will be used.}

\item{\code{job_name}}{(str): Name of the training job to be created. If not
specified, one is generated, using the base name given to the
constructor if applicable.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Object2Vec$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
